import os
from typing import List

from crewai.tools import BaseTool
from openai import OpenAI
from pydantic import BaseModel, Field
from pymilvus import Collection, connections

from ..embedding import to_vector
from .redis_store import commit_summary_chunk, xadd_alert

_milvus_loaded = False
_collection = None


class SearchMilvusTool(BaseTool):
    name: str = "search_milvus"
    description: str = "åœ¨ Milvus ä¸­æœå°‹ COPD ç›¸é—œå•ç­”ï¼Œå›å‚³ç›¸ä¼¼å•é¡Œèˆ‡ç­”æ¡ˆ"

    def _run(self, query: str) -> str:
        global _milvus_loaded, _collection
        try:
            if not _milvus_loaded:
                try:
                    connections.get_connection("default")
                except Exception:
                    connections.connect(
                        alias="default",
                        uri=os.getenv("MILVUS_URI", "http://localhost:19530"),
                    )
                _collection = Collection("copd_qa")
                _collection.load()
                _milvus_loaded = True
            thr = float(os.getenv("SIMILARITY_THRESHOLD", 0.7))
            vec = to_vector(query)
            if not isinstance(vec, list):
                vec = vec.tolist() if hasattr(vec, "tolist") else list(vec)
            res = _collection.search(
                data=[vec],
                anns_field="embedding",
                param={"metric_type": "COSINE", "params": {"nprobe": 10}},
                limit=5,
                output_fields=["question", "answer", "category"],
            )
            out: List[str] = []
            for hit in res[0]:
                if hit.score >= thr:
                    q = hit.entity.get("question")
                    a = hit.entity.get("answer")
                    cat = hit.entity.get("category")
                    out.append(f"[{cat}] (ç›¸ä¼¼åº¦: {hit.score:.3f})\nQ: {q}\nA: {a}")
            return "\n\n".join(out) if out else "[æŸ¥ç„¡é«˜ç›¸ä¼¼åº¦çµæœ]"
        except Exception as e:
            return f"[Milvus éŒ¯èª¤] {e}"


def summarize_chunk_and_commit(
    user_id: str, start_round: int, history_chunk: list
) -> bool:
    if not history_chunk:
        return True
    text = "".join(
        [
            f"ç¬¬{start_round + i + 1}è¼ª:\né•·è¼©: {h['input']}\né‡‘å­«: {h['output']}\n\n"
            for i, h in enumerate(history_chunk)
        ]
    )
    prompt = f"è«‹å°‡ä¸‹åˆ—å°è©±åš 80-120 å­—æ‘˜è¦ï¼Œèšç„¦ï¼šå¥åº·å•é¡Œã€æƒ…ç·’ã€ç”Ÿæ´»è¦é»ã€‚\n\n{text}"
    try:
        client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        res = client.chat.completions.create(
            model=os.getenv("MODEL_NAME", "gpt-4o-mini"),
            messages=[
                {"role": "system", "content": "ä½ æ˜¯å°ˆæ¥­çš„å°è©±æ‘˜è¦åŠ©æ‰‹ã€‚"},
                {"role": "user", "content": prompt},
            ],
            temperature=0.3,
        )
        body = (res.choices[0].message.content or "").strip()
        header = f"--- ç¬¬{start_round + 1}è‡³{start_round + len(history_chunk)}è¼ªå°è©±æ‘˜è¦ ---\n"
        return commit_summary_chunk(
            user_id,
            expected_cursor=start_round,
            advance=len(history_chunk),
            add_text=header + body,
        )
    except Exception as e:
        print(f"[æ‘˜è¦éŒ¯èª¤] {e}")
        return False


class AlertCaseManagerToolSchema(BaseModel):
    reason: str = Field(
        ...,
        description="ä¸€è¡ŒåŸå› å­—ä¸²ï¼Œéœ€ä»¥ 'EMERGENCY:' é–‹é ­ï¼Œä¾‹å¦‚ï¼š'EMERGENCY: suicidal ideation'",
    )


class AlertCaseManagerTool(BaseTool):
    name: str = "alert_case_manager"
    description: str = (
        "åµæ¸¬åˆ°ç·Šæ€¥å¥åº·/å¿ƒç†é¢¨éšªæ™‚ï¼Œç«‹å³é€šå ±å€‹ç®¡å¸«ã€‚"
        "ã€ç”¨æ³•ã€‘ä»¥ JSON å‚³å…¥ {'reason': 'EMERGENCY: <æ¥µç°¡åŸå› >'}ï¼›"
        "ç”¨æˆ¶IDç”±ç³»çµ±è‡ªå‹•å¡«å…¥ï¼Œç„¡éœ€æä¾›ã€‚"
    )
    args_schema = AlertCaseManagerToolSchema  # â˜… é—œéµï¼šæ˜ç¢ºå®£å‘Šåƒæ•¸éµ

    def _run(self, reason: str) -> str:
        try:
            uid = self.runtime_context.get("user_id") or os.getenv("CURRENT_USER_ID")
            import datetime

            ts = datetime.datetime.now().isoformat(timespec="seconds")
            print(
                f"[{ts}] ğŸš¨ AlertCaseManagerTool triggered: user={uid}, reason={reason}"
            )
            from .redis_store import xadd_alert

            xadd_alert(uid, reason)
            return f"âš ï¸ å·²é€šå ±å€‹ç®¡å¸«ä½¿ç”¨è€…ID: {uid}ï¼Œäº‹ç”±ï¼š{reason}"
        except Exception as e:
            return f"[Alert é€å‡ºå¤±æ•—] {e}"


class ModelGuardrailTool(BaseTool):
    name: str = "model_guardrail"
    description: str = (
        "å®Œå…¨ç”± LLM å®‰å…¨å¯©æŸ¥ï¼šåˆ¤æ–·æ˜¯å¦é•æ³•/å±éšª/è‡ªå‚·ï¼Œæˆ–å±¬æ–¼éœ€å°ˆæ¥­äººå£«çš„å…·é«”æŒ‡ç¤ºï¼ˆå¦‚ç”¨è—¥/åŠ‘é‡/è¨ºæ–·/è™•ç½®ï¼‰ã€‚åªè¼¸å‡º OK æˆ– BLOCK: <åŸå› >ã€‚"
    )

    def _run(self, text: str) -> str:
        try:
            client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
            guard_model = os.getenv(
                "GUARD_MODEL", os.getenv("MODEL_NAME", "gpt-4o-mini")
            )
            sys = (
                "ä½ æ˜¯å®‰å…¨å¯©æŸ¥å™¨ï¼Œéœ€ç²¾æº–åˆ¤æ–·æ˜¯å¦å¿…é ˆæ””æˆªã€‚\n"
                "ã€å…è¨±ï¼ˆå› OKï¼‰ã€‘ä»¥ä¸‹æƒ…æ³ä¸€å¾‹æ”¾è¡Œï¼š\n"
                "- ç—‡ç‹€æ•˜è¿°æˆ–ä¸€èˆ¬å¥åº·è³‡è¨Šçš„è©¢å•ï¼ˆä¾‹å¦‚ï¼šç—‡ç‹€ã€æˆå› ã€é¢¨éšªå› å­ã€ä½•æ™‚å°±é†«çš„æé†’ï¼‰ã€‚\n"
                "- ä¸€èˆ¬ç”Ÿæ´»ä¿å¥/è¡›æ•™/æ³¨æ„äº‹é …ç­‰éå€‹æ¡ˆåŒ–ã€éæŒ‡ä»¤æ€§çš„å»ºè­°ã€‚\n"
                "- æƒ…ç·’ä½è½ã€è‡ªæ®ºå¿µé ­æˆ–æ±‚åŠ©çš„è¡¨é”ï¼ˆä¸å«å…·é«”æ–¹æ³•ï¼‰ã€‚\n"
                "ã€å¿…é ˆæ””æˆªï¼ˆå› BLOCK: <åŸå› >ï¼‰ã€‘ç¬¦åˆä»»ä¸€æ¢ä»¶ï¼š\n"
                "1) é•æ³•/å±éšªè¡Œç‚ºçš„æ•™å­¸ã€è²·è³£ã€è£½ä½œæˆ–è¦é¿ï¼ˆæ¯’å“ã€æ­¦å™¨ã€æš´åŠ›ã€é§­å®¢ã€çˆ†è£‚ç‰©ç­‰ï¼‰ã€‚\n"
                "2) è‡ªå‚·/ä»–å‚·/è‡ªæ®º/è‡ªæ®˜çš„ã€å…·é«”æ–¹æ³•æŒ‡å°æˆ–é¼“å‹µåŸ·è¡Œã€ã€‚\n"
                "3) æˆäººæ€§å…§å®¹æˆ–æœªæˆå¹´ç›¸é—œä¸ç•¶å…§å®¹çš„è«‹æ±‚ã€‚\n"
                "4) é†«ç™‚/ç”¨è—¥/åŠ‘é‡/è¨ºæ–·/è™•ç½®ç­‰ã€å…·é«”ã€å€‹æ¡ˆåŒ–ã€å¯åŸ·è¡Œã€çš„å°ˆæ¥­æŒ‡ç¤ºæˆ–æ–¹æ¡ˆã€‚\n"
                "5) æ³•å¾‹ã€æŠ•è³‡ã€ç¨…å‹™ç­‰é«˜é¢¨éšªé ˜åŸŸä¹‹ã€å…·é«”ã€å¯åŸ·è¡Œã€çš„å°ˆæ¥­æŒ‡å°ã€‚\n"
                "ã€åˆ¤æ–·åŸå‰‡ã€‘åƒ…åœ¨è«‹æ±‚æ˜ç¢ºè½å…¥ä¸Šè¿°æ””æˆªæ¢ä»¶æ™‚æ‰ BLOCKï¼›\n"
                "è‹¥æ˜¯æè¿°ç‹€æ³æˆ–å°‹æ±‚ä¸€èˆ¬æ€§èªªæ˜/ä¿å¥å»ºè­°ï¼Œè«‹å› OKã€‚\n"
                "è‹¥ä¸ç¢ºå®šï¼Œé è¨­å› OKã€‚\n"
                "ã€è¼¸å‡ºæ ¼å¼ã€‘åªèƒ½æ˜¯ï¼š\n"
                "OK\n"
                "æˆ–\n"
                "BLOCK: <æ¥µç°¡åŸå› >\n"
            )

            user = f"ä½¿ç”¨è€…è¼¸å…¥ï¼š{text}\nè«‹ä¾è¦å‰‡åªè¼¸å‡º OK æˆ– BLOCK: <åŸå› >ã€‚"
            res = client.chat.completions.create(
                model=guard_model,
                messages=[
                    {"role": "system", "content": sys},
                    {"role": "user", "content": user},
                ],
                temperature=0,
                max_tokens=24,
            )
            out = (res.choices[0].message.content or "").strip()
            # é è¨­å¯¬é¬†é€šéï¼šè‹¥éæ˜ç¢º BLOCKï¼Œä¸€å¾‹è¦–ç‚º OK
            if not out.startswith("BLOCK:"):
                return "OK"
            # åƒ…ä¿ç•™ç²¾ç°¡ BLOCK ç†ç”±
            if len(out) > 256:
                out = out[:256]
            return out
        except Exception as e:
            # Guardrail æ•…éšœæ™‚ï¼Œä¸è¦é˜»æ“‹ä¸»æµç¨‹
            print(f"[guardrail_error] {e}")
            return "OK"
